{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3c6746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087b0da0",
   "metadata": {},
   "source": [
    "## Web Scraping the Ryman Calendar\n",
    "\n",
    "In this exercise, your objective is to use BeautifulSoup in order to obtain a dataset of upcoming events at the Ryman. This information is available at https://ryman.com/events/, but you will take the contents of this website and convert it into a pandas DataFrame.\n",
    "\n",
    "The website splits the events across multiple pages, but start by just working on the first page. Later on in the exercise, you'll take what you've done for the first page and apply it across other pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dabc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://ryman.com/events/'\n",
    "response = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186796d6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b572a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BS(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a11046",
   "metadata": {},
   "source": [
    "#### Note on .text vs .content\n",
    "response.text is the content of the response in Unicode, and response.content is the content of the response in bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a11d39",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "Start by using either the inspector or by viewing the page source. Can you identify a tag that might be helpful for finding the names of all performers? For now, just worry about the headliner and don't worry about the opener. (Eg. For Vince Gill, featuring Wendy Moten, we only care about Vince Gill.) Make use of this to create a list containing just the names of each inductee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f36b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = soup.find_all('a', attrs={'class' : 'tribe-event-url'})\n",
    "titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fac9eb",
   "metadata": {},
   "source": [
    "Deme's code\n",
    "\n",
    "for artist in soup.find_all(\"a\", class_=\"tribe-event-url\"):\n",
    "    print(artist.get('title'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e78de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "headliners = [x.get('title') for x in titles]\n",
    "headliners\n",
    "#Chris's code: [x['title'] for x in titles]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2118f1c0",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "Next, try and find a tag that could be used to find the date and time for each show. Extract these into two lists, one containing the date and the other containing the time. (Eg. THURSDAY, AUGUST 4, 2022 AT 8:00 PM CDT should be split into August 4, 2022 and 8:00 PM CDT.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42007efa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_datetimes = soup.find_all('time')\n",
    "time_datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964f74cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datetimes = pd.DataFrame(list(time_datetimes))\n",
    "\n",
    "df_datetimes.columns = ['show_date_time']\n",
    "\n",
    "df_datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42171b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = df_datetimes['show_date_time'].str.extract(\"\\sat\\s(.+)\")\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcbb751",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = df_datetimes['show_date_time'].str.extract(\"(.+)\\sat\\s\")\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c3fb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "headliners_df = pd.DataFrame(list(headliners))\n",
    "headliners_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f0229b",
   "metadata": {},
   "source": [
    "#### Chris's code:\n",
    "[x.find('time') for x in time_soup if x.find('time)]\n",
    "\n",
    "He did an extra step to pull p tags, so he got some \"None\" values initially when he pulled times out of the time_soup, this little trick with the list comprehension says to only keep values when there IS an actual value, skips over the \"None\"s\n",
    "\n",
    "[x.find('time').text for x in time_soup if x.find('time)]\n",
    "\n",
    "The version above gets him straight to the text component\n",
    "\n",
    "date_list = []\n",
    "\n",
    "hour_list = []\n",
    "\n",
    "for time in time_list:\n",
    "\n",
    "        x = time.split(' at ')\n",
    "        \n",
    "        date_list.append(x[0])\n",
    "        \n",
    "        hour_list.append(x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abd80be",
   "metadata": {},
   "source": [
    "After seeing Chris's solution, I wanted to try out doing things with lists rather than dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447413ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time_list = [re.search(\"\\sat\\s(.+)<\", str(x))[1] for x in time_datetimes]\n",
    "time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9411c5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "date_list = [re.search(\">(.+)\\sat\\s\", str(x))[1] for x in time_datetimes]\n",
    "date_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dc90ec",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "Take the two lists you created on parts 1 and 2 and convert it into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1144c1fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_info1 = pd.concat([headliners_df, dates, times], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1394d6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_info1.columns = ['Headliner', 'Date', 'Time']\n",
    "show_info1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740d3434",
   "metadata": {},
   "source": [
    "Playing with getting to the final dataframe in fewer steps below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee280a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "headliners_df = pd.DataFrame(list([x.get('title') for x in soup.findAll('a', attrs={'class' : 'tribe-event-url'})]))\n",
    "\n",
    "df_datetimes = pd.DataFrame(list(soup.findAll('time'))).rename(columns = {0 : 'show_date_time'})\n",
    "\n",
    "times = df_datetimes['show_date_time'].str.extract(\"\\sat\\s(.+)\")\n",
    "\n",
    "dates = df_datetimes['show_date_time'].str.extract(\"(.+)\\sat\\s\")\n",
    "\n",
    "show_info2 = pd.concat([headliners_df, dates, times], axis=1)\n",
    "show_info2.columns = ['Headliner', 'Date', 'Time']\n",
    "\n",
    "show_info2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aa1abb",
   "metadata": {},
   "source": [
    "#### Chris's code\n",
    "\n",
    "page_1_df = pd.DataFrame({'artist' : artist_list, 'date' : date_list, 'time' : hour_list}]\n",
    "\n",
    "He left his as lists and combined those lists into a dataframe directly. The method above shows using a dictionary to define the column name each list will have once it's in the df."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff86d7fb",
   "metadata": {},
   "source": [
    "Using the lists I created after seeing Chris's solution to make a df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646dbf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg1_df = pd.DataFrame({'Headliner' : list(headliners), 'Date' : date_list, 'Time' : time_list})\n",
    "pg1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890cd3cd",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "Now, you need to take what you created for the first page and apply it across multiple rest of the pages so that you can scrape all inductees. Notice how the url changes when you click the \"More Events\" button at the top of the page. Check that the code that you wrote for the first page still works for page 2. Once you have verified that your code will still work, write a for loop that will cycle through the first five pages of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c531da",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL2 = 'https://ryman.com/events/list/?tribe_event_display=list&tribe_paged=2'\n",
    "response2 = requests.get(URL2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee1815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2 = BS(response2.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9818ba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "headliners2 = pd.DataFrame(list([x.get('title') for x in soup2.findAll('a', attrs={'class' : 'tribe-event-url'})]))\n",
    "headliners2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fd9d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimes2 = pd.DataFrame(list(soup2.findAll('time'))).rename(columns = {0 : 'show_date_time'})\n",
    "datetimes2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b1f219",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 6):\n",
    "    if i == 1:\n",
    "        all_soup = BS(requests.get('https://ryman.com/events/').text)\n",
    "    else:\n",
    "        all_soup.body.append(BS(requests.get(f'https://ryman.com/events/list/?tribe_event_display=list&tribe_paged={i}').text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e3bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df82d176",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_headliners = pd.DataFrame(list([x.get('title') for x in all_soup.findAll('a', attrs={'class' : 'tribe-event-url'})]))\n",
    "\n",
    "all_datetimes = pd.DataFrame(list(all_soup.findAll('time'))).rename(columns = {0 : 'show_date_time'})\n",
    "\n",
    "all_times = all_datetimes['show_date_time'].str.extract(\"\\sat\\s(.+)\")\n",
    "\n",
    "all_dates = all_datetimes['show_date_time'].str.extract(\"(.+)\\sat\\s\")\n",
    "\n",
    "all_show_info= pd.concat([all_headliners, all_dates, all_times], axis=1)\n",
    "all_show_info.columns = ['Headliner', 'Date', 'Time']\n",
    "\n",
    "all_show_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd981303",
   "metadata": {},
   "source": [
    "#### Chris's code\n",
    "\n",
    "main_artist_list = []\n",
    "main_date_list = []\n",
    "main_time_list = []\n",
    "\n",
    "URL2 = 'https://ryman.com/events/list/?tribe_event_display=list&tribe_paged='\n",
    "\n",
    "for num in range (1, 6):\n",
    "\n",
    "        new_url = URL2 + str(num)\n",
    "        response = requests.get(new_url)\n",
    "        soup = BS(response.content)\n",
    "        \n",
    "        artist_soup = soup.find_all('a', attrs = {'class' : 'tribe-event-url'})\n",
    "        artist_list = [x['title'] for x in artist_soup]\n",
    "        main_artist_list.extend(artist_list) #append would make a list of lists in this case, extend just adds to the list\n",
    "        \n",
    "        time_soup = soup.find_all('p')\n",
    "        time_list = [x.find('time').text for x in time_soup if x.find('time')]\n",
    "        \n",
    "        for time in time_list\n",
    "            z = time.split(' at ')\n",
    "            main_date_list.append(x[0])\n",
    "            main_time_list.append(x[1])\n",
    "            \n",
    "main_df = pd.DataFrame({'artist' : main_artist_list, 'date' : main_date_list, 'time': main_time_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36123d7",
   "metadata": {},
   "source": [
    "#### Bonus 1\n",
    "Add to your data frame the opening act for all shows that list an opener."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68684e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "div = all_soup.findAll('div', attrs={'class' : 'tribe-beside-image'})\n",
    "div\n",
    "#Need to do this so that when there are multiple instances of using opener for a single show, they are within the same block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc60fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(div[1].find_all(\"span\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e96653",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_openers = []\n",
    "clean_openers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9317409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_openers = []\n",
    "for x in div:\n",
    "    if len(x.find_all(\"span\")) != 0:\n",
    "        if re.search(\"with.+\", str(x.find_all(\"span\")[-1])):\n",
    "            clean_openers.append(x.find_all(\"span\")[-1].text)\n",
    "        elif re.search(\"With.+\", str(x.find_all(\"span\")[-1])):\n",
    "            clean_openers.append(x.find_all(\"span\")[-1].text)\n",
    "        elif re.search(\"featuring.+\", str(x.find_all(\"span\")[-1])):\n",
    "            clean_openers.append(x.find_all(\"span\")[-1].text)\n",
    "        else:\n",
    "            clean_openers.append(\"No Opener\")\n",
    "    else:\n",
    "            clean_openers.append(\"No Opener\")\n",
    "\n",
    "clean_openers_df = pd.DataFrame(clean_openers)\n",
    "\n",
    "clean_openers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c70ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_show_info= pd.concat([all_headliners, clean_openers_df, all_dates, all_times], axis=1)\n",
    "all_show_info.columns = ['Headliner', 'Opener', 'Date', 'Time']\n",
    "all_show_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d984de92",
   "metadata": {},
   "source": [
    "#### Bonus 2\n",
    "If you click the \"MORE INFO\" button for an event, it will take you to a page which shows ticket prices. Write code that can be used to retrieve the ticket prices for each show that you have scraped. Make sure that your code can handle cases where the show has been canceled (eg. https://ryman.com/event/nhabit-worship-experience/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b709de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "buttons = all_soup.findAll('a', attrs={'class' : 'smallblackbutton'})\n",
    "buttons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adae9578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "links = [x.get('href') for x in buttons]\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7e9506",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prices = []\n",
    "for l in links:\n",
    "    if re.search('canceled', str(BS(requests.get(l).text).find('strong', attrs={'class' : 'show-status-label'}))):\n",
    "        prices.append(\"Show Cancelled\")\n",
    "    elif re.search(\"\\w\", str(BS(requests.get(l).text).find_all('p', attrs={'class' : 'theprices'}))):\n",
    "        prices.append(BS(requests.get(l).text).find('p', attrs={'class' : 'theprices'}).text)\n",
    "    else:\n",
    "        prices.append(\"See website for details\")\n",
    "    \n",
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d506a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing this so that the $ show up properly in the dataframe\n",
    "prices_escape = [i.replace(\"$\", \"\\$\") for i in prices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce1fe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df = pd.DataFrame(prices_escape)\n",
    "price_df.columns = ['Pricing Information']\n",
    "price_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ab51a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_show_info = pd.concat([all_show_info, price_df], axis=1)\n",
    "final_show_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e62adeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
